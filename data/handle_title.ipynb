{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('max_columns', 100)\n",
    "pd.set_option('max_rows', 100)\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc='pandas bar')\n",
    "import pickle\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "data_path = r'./data_for_ctr_predict/'\n",
    "with open(data_path+'doc_info.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "item = []\n",
    "for line in tqdm(lines):\n",
    "    unit = line.split('\\t')\n",
    "    if len(unit)<7:\n",
    "        for _ in range(7-len(unit)):\n",
    "            unit.append(np.nan)\n",
    "    item.append(unit)\n",
    "#item = np.array(item)\n",
    "item = pd.DataFrame(item, columns=['item_id', 'item_title', 'item_time', 'item_picture', 'item_cluster1', 'item_cluster2', 'item_keywords'])\n",
    "item['item_id'] = item['item_id'].astype('int64')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 633391/633391 [00:02<00:00, 289165.89it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 对每个doc的title使用Bert进行embedding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# 构建tokenizer和model\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-chinese\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import torch\n",
    "title_list = item.item_title.values\n",
    "\n",
    "model.to('cuda:2')\n",
    "res = torch.tensor([])\n",
    "\n",
    "bs = 1000\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(int(title_list.shape[0] / bs) + 1)):\n",
    "        batch = title_list[i * bs: (i + 1) * bs]\n",
    "        input = tokenizer(list(batch), padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "        input.to('cuda:2')\n",
    "        output = model(**input)\n",
    "        res = torch.cat([res, output['pooler_output'].cpu().detach()], dim=0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 633/633 [07:33<00:00,  1.40it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "res.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([633391, 768])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "res = res.cpu().detach().numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "with open(r'title_768.pkl', 'wb') as f:\n",
    "    pickle.dump(res, f, pickle.HIGHEST_PROTOCOL)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "with open(r'title_768.pkl', 'rb') as f:\n",
    "    res = pickle.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# 做降维\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_dim = 16\n",
    "transformer = PCA(n_components=n_dim)\n",
    "res = transformer.fit_transform(res)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open(r'title_' + str(n_dim) + '.pkl', 'wb') as f:\n",
    "    pickle.dump(res, f, pickle.HIGHEST_PROTOCOL)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('rec': conda)"
  },
  "interpreter": {
   "hash": "9c7f61cd7af8a91025d9808f8ebc6c0f0cafdddc4c9ffe8ba00f0e1268577df3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}